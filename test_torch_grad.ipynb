{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1., 2., 3.]), tensor([4., 5., 6.]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor([1, 2, 3], dtype=torch.float32)\n",
    "b = torch.tensor([4, 5, 6], dtype=torch.float32)\n",
    "a, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.2673, 0.5345, 0.8018]]), tensor([[0.4558, 0.5698, 0.6838]]))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aa = torch.nn.functional.normalize(a.unsqueeze(0))\n",
    "ba = torch.nn.functional.normalize(b.unsqueeze(0))\n",
    "aa, ba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1886, -0.0353,  0.1180]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aa - ba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1886, -0.0353,  0.1180]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aa.sub(ba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.2252)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aa.sub(ba).norm(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1126)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aa.sub(ba).norm(2).div(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1129)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aa.sub(ba).norm(2).div(2).arcsin()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(7.)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(2., requires_grad=True)\n",
    "\n",
    "a = torch.add(x, 1)\n",
    "b = torch.add(x, 2)\n",
    "y = torch.mul(a, b)\n",
    "\n",
    "y.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "requires_grad:  True True True True\n",
      "is_leaf:  True False False False\n",
      "grad:  tensor(7.) None None None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_12627/3011949837.py:3: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at aten/src/ATen/core/TensorBody.h:489.)\n",
      "  print(\"grad: \", x.grad, a.grad, b.grad, y.grad)\n"
     ]
    }
   ],
   "source": [
    "print(\"requires_grad: \", x.requires_grad, a.requires_grad, b.requires_grad, y.requires_grad)\n",
    "print(\"is_leaf: \", x.is_leaf, a.is_leaf, b.is_leaf, y.is_leaf)\n",
    "print(\"grad: \", x.grad, a.grad, b.grad, y.grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(7.)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(7.),)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor(2., requires_grad=True)\n",
    "\n",
    "a = torch.add(x, 1)\n",
    "b = torch.add(x, 2)\n",
    "y = torch.mul(a, b)\n",
    "\n",
    "grad = torch.autograd.grad(outputs=y, inputs=x)\n",
    "print(grad[0])\n",
    "grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(12.) tensor(4.)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(2.).requires_grad_()\n",
    "y = torch.tensor(3.).requires_grad_()\n",
    "\n",
    "z = x * x * y\n",
    "\n",
    "grad_x = torch.autograd.grad(outputs=z, inputs=x, retain_graph=True)\n",
    "grad_y = torch.autograd.grad(outputs=z, inputs=y)\n",
    "print(grad_x[0], grad_y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/media/mix060514/Data/pj/diffusers-test/test_torch_grad.ipynb Cell 13\u001b[0m line \u001b[0;36m7\n\u001b[1;32m      <a href='vscode-notebook-cell:/media/mix060514/Data/pj/diffusers-test/test_torch_grad.ipynb#X30sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m z \u001b[39m=\u001b[39m x \u001b[39m*\u001b[39m x \u001b[39m*\u001b[39m y\n\u001b[1;32m      <a href='vscode-notebook-cell:/media/mix060514/Data/pj/diffusers-test/test_torch_grad.ipynb#X30sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m grad_x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mgrad(outputs\u001b[39m=\u001b[39mz, inputs\u001b[39m=\u001b[39mx, retain_graph\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/media/mix060514/Data/pj/diffusers-test/test_torch_grad.ipynb#X30sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m grad_xx \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mgrad(outputs\u001b[39m=\u001b[39;49mgrad_x, inputs\u001b[39m=\u001b[39;49mx)\n\u001b[1;32m      <a href='vscode-notebook-cell:/media/mix060514/Data/pj/diffusers-test/test_torch_grad.ipynb#X30sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mprint\u001b[39m(grad_x[\u001b[39m0\u001b[39m], grad_xx[\u001b[39m0\u001b[39m])\n",
      "File \u001b[0;32m/media/mix060514/Data/pj/diffusers-test/venv/lib/python3.10/site-packages/torch/autograd/__init__.py:411\u001b[0m, in \u001b[0;36mgrad\u001b[0;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched, materialize_grads)\u001b[0m\n\u001b[1;32m    407\u001b[0m     result \u001b[39m=\u001b[39m _vmap_internals\u001b[39m.\u001b[39m_vmap(vjp, \u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m, allow_none_pass_through\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)(\n\u001b[1;32m    408\u001b[0m         grad_outputs_\n\u001b[1;32m    409\u001b[0m     )\n\u001b[1;32m    410\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 411\u001b[0m     result \u001b[39m=\u001b[39m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    412\u001b[0m         t_outputs,\n\u001b[1;32m    413\u001b[0m         grad_outputs_,\n\u001b[1;32m    414\u001b[0m         retain_graph,\n\u001b[1;32m    415\u001b[0m         create_graph,\n\u001b[1;32m    416\u001b[0m         inputs,\n\u001b[1;32m    417\u001b[0m         allow_unused,\n\u001b[1;32m    418\u001b[0m         accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    419\u001b[0m     )  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    420\u001b[0m \u001b[39mif\u001b[39;00m materialize_grads:\n\u001b[1;32m    421\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39many\u001b[39m(\n\u001b[1;32m    422\u001b[0m         result[i] \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_tensor_like(inputs[i])\n\u001b[1;32m    423\u001b[0m         \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(inputs))\n\u001b[1;32m    424\u001b[0m     ):\n",
      "\u001b[0;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(2.).requires_grad_()\n",
    "y = torch.tensor(3.).requires_grad_()\n",
    "\n",
    "z = x * x * y\n",
    "\n",
    "grad_x = torch.autograd.grad(outputs=z, inputs=x, retain_graph=True)\n",
    "grad_xx = torch.autograd.grad(outputs=grad_x, inputs=x)\n",
    "print(grad_x[0], grad_xx[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(12., grad_fn=<AddBackward0>) tensor(6.)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(2.).requires_grad_()\n",
    "y = torch.tensor(3.).requires_grad_()\n",
    "\n",
    "z = x * x * y\n",
    "\n",
    "grad_x = torch.autograd.grad(outputs=z, inputs=x, create_graph=True)\n",
    "grad_xx = torch.autograd.grad(outputs=grad_x, inputs=x)\n",
    "print(grad_x[0], grad_xx[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(6.)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(2.).requires_grad_()\n",
    "y = torch.tensor(3.).requires_grad_()\n",
    "\n",
    "z = x * x * y\n",
    "\n",
    "grad = torch.autograd.grad(outputs=z, inputs=x, create_graph=True)\n",
    "grad[0].backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2., requires_grad=True) tensor(12., grad_fn=<AddBackward0>) tensor(6.)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(2.).requires_grad_()\n",
    "y = torch.tensor(3.).requires_grad_()\n",
    "\n",
    "z = x * x * y\n",
    "\n",
    "z.backward(create_graph=True)\n",
    "grad_xx = torch.autograd.grad(outputs=x.grad, inputs=x)\n",
    "print(x, grad_x[0], grad_xx[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2., requires_grad=True) tensor(18., grad_fn=<CopyBackwards>)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(2.).requires_grad_()\n",
    "y = torch.tensor(3.).requires_grad_()\n",
    "\n",
    "z = x * x * y\n",
    "\n",
    "z.backward(create_graph=True)\n",
    "x.grad.backward()\n",
    "print(x, x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2., requires_grad=True) tensor(6., grad_fn=<CopyBackwards>)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(2.).requires_grad_()\n",
    "y = torch.tensor(3.).requires_grad_()\n",
    "\n",
    "z = x * x * y\n",
    "\n",
    "z.backward(create_graph=True)\n",
    "x.grad.data.zero_()\n",
    "x.grad.backward()\n",
    "print(x, x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "grad can be implicitly created only for scalar outputs",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/media/mix060514/Data/pj/diffusers-test/test_torch_grad.ipynb Cell 19\u001b[0m line \u001b[0;36m4\n\u001b[1;32m      <a href='vscode-notebook-cell:/media/mix060514/Data/pj/diffusers-test/test_torch_grad.ipynb#X31sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor([\u001b[39m1.\u001b[39m, \u001b[39m2.\u001b[39m, \u001b[39m3.\u001b[39m], requires_grad\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/media/mix060514/Data/pj/diffusers-test/test_torch_grad.ipynb#X31sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m y \u001b[39m=\u001b[39m x \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/media/mix060514/Data/pj/diffusers-test/test_torch_grad.ipynb#X31sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m y\u001b[39m.\u001b[39;49mbackward()\n",
      "File \u001b[0;32m/media/mix060514/Data/pj/diffusers-test/venv/lib/python3.10/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    523\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    524\u001b[0m )\n",
      "File \u001b[0;32m/media/mix060514/Data/pj/diffusers-test/venv/lib/python3.10/site-packages/torch/autograd/__init__.py:259\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    250\u001b[0m inputs \u001b[39m=\u001b[39m (\n\u001b[1;32m    251\u001b[0m     (inputs,)\n\u001b[1;32m    252\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(inputs, (torch\u001b[39m.\u001b[39mTensor, graph\u001b[39m.\u001b[39mGradientEdge))\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    255\u001b[0m     \u001b[39melse\u001b[39;00m \u001b[39mtuple\u001b[39m()\n\u001b[1;32m    256\u001b[0m )\n\u001b[1;32m    258\u001b[0m grad_tensors_ \u001b[39m=\u001b[39m _tensor_or_tensors_to_tuple(grad_tensors, \u001b[39mlen\u001b[39m(tensors))\n\u001b[0;32m--> 259\u001b[0m grad_tensors_ \u001b[39m=\u001b[39m _make_grads(tensors, grad_tensors_, is_grads_batched\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m    260\u001b[0m \u001b[39mif\u001b[39;00m retain_graph \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n",
      "File \u001b[0;32m/media/mix060514/Data/pj/diffusers-test/venv/lib/python3.10/site-packages/torch/autograd/__init__.py:132\u001b[0m, in \u001b[0;36m_make_grads\u001b[0;34m(outputs, grads, is_grads_batched)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[39mif\u001b[39;00m out\u001b[39m.\u001b[39mrequires_grad:\n\u001b[1;32m    131\u001b[0m     \u001b[39mif\u001b[39;00m out\u001b[39m.\u001b[39mnumel() \u001b[39m!=\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m--> 132\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m    133\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mgrad can be implicitly created only for scalar outputs\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    134\u001b[0m         )\n\u001b[1;32m    135\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m out\u001b[39m.\u001b[39mdtype\u001b[39m.\u001b[39mis_floating_point:\n\u001b[1;32m    136\u001b[0m         msg \u001b[39m=\u001b[39m (\n\u001b[1;32m    137\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mgrad can be implicitly created only for real scalar outputs\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    138\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m but got \u001b[39m\u001b[39m{\u001b[39;00mout\u001b[39m.\u001b[39mdtype\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    139\u001b[0m         )\n",
      "\u001b[0;31mRuntimeError\u001b[0m: grad can be implicitly created only for scalar outputs"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([1., 2., 3.], requires_grad=True)\n",
    "y = x + 1\n",
    "\n",
    "y.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1.])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([1., 2., 3.], requires_grad=True)\n",
    "y = x + 1\n",
    "\n",
    "y.sum().backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2., 4., 6.])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([1., 2., 3.], requires_grad=True)\n",
    "y = x * x\n",
    "\n",
    "y.sum().backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2., 4., 6.])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([1., 2., 3.], requires_grad=True)\n",
    "y: torch.Tensor  = x * x\n",
    "\n",
    "y.backward(gradient=torch.ones_like(x)) # for non-scalar output\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([2., 4., 6.]),)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([1., 2., 3.], requires_grad=True)\n",
    "y: torch.Tensor  = x * x\n",
    "\n",
    "grad_x = torch.autograd.grad(outputs=y, inputs=x, grad_outputs=torch.ones_like(y)) \n",
    "print(grad_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([2., 4., 6.]),)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([1., 2., 3.], requires_grad=True)\n",
    "y: torch.Tensor  = x * x\n",
    "\n",
    "grad_x = torch.autograd.grad(outputs=y.sum(), inputs=x) \n",
    "print(grad_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "requires_grad:  True False True True\n",
      "is_leaf:  True True False False\n",
      "grad:  tensor([3.]) None None None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_20512/2370103711.py:11: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at aten/src/ATen/core/TensorBody.h:489.)\n",
      "  print(\"grad: \", x.grad, a.grad, b.grad, y.grad)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([2.], requires_grad=True)\n",
    "\n",
    "a = torch.add(x, 1).detach()\n",
    "b = torch.add(x, 2)\n",
    "y = torch.mul(a, b)\n",
    "\n",
    "y.backward()\n",
    "\n",
    "print(\"requires_grad: \", x.requires_grad, a.requires_grad, b.requires_grad, y.requires_grad)\n",
    "print(\"is_leaf: \", x.is_leaf, a.is_leaf, b.is_leaf, y.is_leaf)\n",
    "print(\"grad: \", x.grad, a.grad, b.grad, y.grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "Dimension out of range (expected to be in range of [-2, 1], but got 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mUntitled-1.ipynb Cell 4\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:Untitled-1.ipynb?jupyter-notebook#W6sdW50aXRsZWQ%3D?line=0'>1</a>\u001b[0m torch\u001b[39m.\u001b[39;49mnn\u001b[39m.\u001b[39;49mfunctional\u001b[39m.\u001b[39;49mnormalize(a\u001b[39m.\u001b[39;49munsqueeze(\u001b[39m0\u001b[39;49m), dim\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m)\n",
      "File \u001b[0;32m/media/mix060514/Data/pj/diffusers-test/venv/lib/python3.10/site-packages/torch/nn/functional.py:4753\u001b[0m, in \u001b[0;36mnormalize\u001b[0;34m(input, p, dim, eps, out)\u001b[0m\n\u001b[1;32m   4751\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(normalize, (\u001b[39minput\u001b[39m, out), \u001b[39minput\u001b[39m, p\u001b[39m=\u001b[39mp, dim\u001b[39m=\u001b[39mdim, eps\u001b[39m=\u001b[39meps, out\u001b[39m=\u001b[39mout)\n\u001b[1;32m   4752\u001b[0m \u001b[39mif\u001b[39;00m out \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 4753\u001b[0m     denom \u001b[39m=\u001b[39m \u001b[39minput\u001b[39;49m\u001b[39m.\u001b[39;49mnorm(p, dim, keepdim\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\u001b[39m.\u001b[39mclamp_min(eps)\u001b[39m.\u001b[39mexpand_as(\u001b[39minput\u001b[39m)\n\u001b[1;32m   4754\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m \u001b[39m/\u001b[39m denom\n\u001b[1;32m   4755\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m/media/mix060514/Data/pj/diffusers-test/venv/lib/python3.10/site-packages/torch/_tensor.py:740\u001b[0m, in \u001b[0;36mTensor.norm\u001b[0;34m(self, p, dim, keepdim, dtype)\u001b[0m\n\u001b[1;32m    736\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    737\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    738\u001b[0m         Tensor\u001b[39m.\u001b[39mnorm, (\u001b[39mself\u001b[39m,), \u001b[39mself\u001b[39m, p\u001b[39m=\u001b[39mp, dim\u001b[39m=\u001b[39mdim, keepdim\u001b[39m=\u001b[39mkeepdim, dtype\u001b[39m=\u001b[39mdtype\n\u001b[1;32m    739\u001b[0m     )\n\u001b[0;32m--> 740\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mnorm(\u001b[39mself\u001b[39;49m, p, dim, keepdim, dtype\u001b[39m=\u001b[39;49mdtype)\n",
      "File \u001b[0;32m/media/mix060514/Data/pj/diffusers-test/venv/lib/python3.10/site-packages/torch/functional.py:1626\u001b[0m, in \u001b[0;36mnorm\u001b[0;34m(input, p, dim, keepdim, out, dtype)\u001b[0m\n\u001b[1;32m   1624\u001b[0m _p \u001b[39m=\u001b[39m \u001b[39m2.0\u001b[39m \u001b[39mif\u001b[39;00m p \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m p\n\u001b[1;32m   1625\u001b[0m \u001b[39mif\u001b[39;00m out \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1626\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mlinalg\u001b[39m.\u001b[39;49mvector_norm(\u001b[39minput\u001b[39;49m, _p, _dim, keepdim, dtype\u001b[39m=\u001b[39;49mdtype)\n\u001b[1;32m   1627\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1628\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mlinalg\u001b[39m.\u001b[39mvector_norm(\u001b[39minput\u001b[39m, _p, _dim, keepdim, dtype\u001b[39m=\u001b[39mdtype, out\u001b[39m=\u001b[39mout)\n",
      "\u001b[0;31mIndexError\u001b[0m: Dimension out of range (expected to be in range of [-2, 1], but got 2)"
     ]
    }
   ],
   "source": [
    "torch.nn.functional.normalize(a.unsqueeze(0), dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1667, 0.3333, 0.5000])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a / a.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.,  0.,  1.])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(a - a.mean()) / a.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0000, 0.5000, 1.0000])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(a - a.min()) / (a.max() - a.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 4., 9.])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.7417)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(a ** 2).sum().sqrt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.7417)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.norm(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.2673, 0.5345, 0.8018])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a / a.norm(2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
